{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediktiv = pd.read_csv('prediktiv_data.csv', index_col = 'id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2930 entries, 1 to 2930\n",
      "Data columns (total 25 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   target       2930 non-null   int64  \n",
      " 1   feature01    2929 non-null   float64\n",
      " 2   feature02    2929 non-null   float64\n",
      " 3   feature03    2930 non-null   int64  \n",
      " 4   feature04    2929 non-null   float64\n",
      " 5   feature05    2930 non-null   int64  \n",
      " 6   feature06    2930 non-null   int64  \n",
      " 7   feature07    2930 non-null   int64  \n",
      " 8   feature08    2930 non-null   int64  \n",
      " 9   feature09    2930 non-null   int64  \n",
      " 10  feature10    2930 non-null   int64  \n",
      " 11  feature10.1  2930 non-null   int64  \n",
      " 12  feature11    198 non-null    object \n",
      " 13  feature12    572 non-null    object \n",
      " 14  feature13    2929 non-null   object \n",
      " 15  feature14    2930 non-null   int64  \n",
      " 16  feature15    1508 non-null   object \n",
      " 17  feature16    2930 non-null   object \n",
      " 18  feature17    2930 non-null   int64  \n",
      " 19  feature18    2930 non-null   int64  \n",
      " 20  feature19    2930 non-null   int64  \n",
      " 21  feature20    2930 non-null   int64  \n",
      " 22  feature21    2930 non-null   int64  \n",
      " 23  feature22    2930 non-null   int64  \n",
      " 24  feature23    2930 non-null   int64  \n",
      "dtypes: float64(3), int64(17), object(5)\n",
      "memory usage: 595.2+ KB\n"
     ]
    }
   ],
   "source": [
    "prediktiv.info()\n",
    "\n",
    "# The dataset exists of 35 columns and 2930 rows.\n",
    "#I can also see that feature11, feature12 and feature15 seems to have less values than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'numpy.str_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emma\\Documents\\Data scientist HT-21\\prediktiv analys\\InlÃ¤mning pa\\Emma_Nilsson_prediktiv_analys_VG.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Emma/Documents/Data%20scientist%20HT-21/prediktiv%20analys/Inl%C3%A4mning%20pa/Emma_Nilsson_prediktiv_analys_VG.ipynb#ch0000005?line=0'>1</a>\u001b[0m prediktiv\u001b[39m.\u001b[39;49mdescribe(\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\environ\\lib\\site-packages\\pandas\\core\\generic.py:10235\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude, datetime_is_numeric)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9985'>9986</a>\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9986'>9987</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdescribe\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9987'>9988</a>\u001b[0m     \u001b[39mself\u001b[39m: NDFrameT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9991'>9992</a>\u001b[0m     datetime_is_numeric\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9992'>9993</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9993'>9994</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9994'>9995</a>\u001b[0m \u001b[39m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=9995'>9996</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10232'>10233</a>\u001b[0m \u001b[39m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10233'>10234</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m> <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10234'>10235</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10235'>10236</a>\u001b[0m         obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10236'>10237</a>\u001b[0m         include\u001b[39m=\u001b[39;49minclude,\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10237'>10238</a>\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10238'>10239</a>\u001b[0m         datetime_is_numeric\u001b[39m=\u001b[39;49mdatetime_is_numeric,\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10239'>10240</a>\u001b[0m         percentiles\u001b[39m=\u001b[39;49mpercentiles,\n\u001b[0;32m  <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/generic.py?line=10240'>10241</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\environ\\lib\\site-packages\\pandas\\core\\describe.py:78\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, datetime_is_numeric, percentiles)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdescribe_ndframe\u001b[39m(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=47'>48</a>\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=48'>49</a>\u001b[0m     obj: NDFrameT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=52'>53</a>\u001b[0m     percentiles: Sequence[\u001b[39mfloat\u001b[39m] \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mndarray \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=53'>54</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=54'>55</a>\u001b[0m     \u001b[39m\"\"\"Describe series or dataframe.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=55'>56</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=56'>57</a>\u001b[0m \u001b[39m    Called from pandas.core.generic.NDFrame.describe()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=75'>76</a>\u001b[0m \u001b[39m    Dataframe or series description.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=76'>77</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=77'>78</a>\u001b[0m     percentiles \u001b[39m=\u001b[39m refine_percentiles(percentiles)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=79'>80</a>\u001b[0m     describer: NDFrameDescriberAbstract\n\u001b[0;32m     <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=80'>81</a>\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\environ\\lib\\site-packages\\pandas\\core\\describe.py:408\u001b[0m, in \u001b[0;36mrefine_percentiles\u001b[1;34m(percentiles)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=404'>405</a>\u001b[0m percentiles \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(percentiles)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=406'>407</a>\u001b[0m \u001b[39m# get them all to be in [0, 1]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=407'>408</a>\u001b[0m validate_percentile(percentiles)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=409'>410</a>\u001b[0m \u001b[39m# median should always be included\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/core/describe.py?line=410'>411</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m percentiles:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\environ\\lib\\site-packages\\pandas\\util\\_validators.py:422\u001b[0m, in \u001b[0;36mvalidate_percentile\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=419'>420</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(q_arr \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=420'>421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=421'>422</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39;49m(\u001b[39m0\u001b[39;49m \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m qs \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m qs \u001b[39min\u001b[39;49;00m q_arr):\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=422'>423</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(q_arr \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=423'>424</a>\u001b[0m \u001b[39mreturn\u001b[39;00m q_arr\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\environ\\lib\\site-packages\\pandas\\util\\_validators.py:422\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=419'>420</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(q_arr \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=420'>421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=421'>422</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m qs \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m qs \u001b[39min\u001b[39;00m q_arr):\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=422'>423</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(q_arr \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Emma/miniconda3/envs/environ/lib/site-packages/pandas/util/_validators.py?line=423'>424</a>\u001b[0m \u001b[39mreturn\u001b[39;00m q_arr\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'numpy.str_'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum the NAN-values in feature11, feature12 and feature15\n",
    "prediktiv['feature11'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediktiv['feature12'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediktiv['feature15'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both feature11, feature12 and feature15 has a lot of NAN-values so I will start by removing them. \n",
    "\n",
    "prediktiv.drop(['feature11', 'feature12', 'feature15'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the categorical features\n",
    "print(prediktiv['feature13'].unique())\n",
    "print(prediktiv['feature16'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create some dummy-variables from thoose features\n",
    "prediktiv = pd.get_dummies(prediktiv, columns =['feature13', 'feature16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is still some NAN-values but I will just replace them with '0' since it is only a few of them in some features\n",
    "print(prediktiv['feature01'].value_counts())\n",
    "prediktiv['feature01'].fillna(0, inplace= True)\n",
    "\n",
    "print(prediktiv['feature02'].value_counts())\n",
    "prediktiv['feature02'].fillna(0, inplace= True)\n",
    "\n",
    "print(prediktiv['feature04'].value_counts())\n",
    "prediktiv['feature04'].fillna(0, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediktiv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I want too search for multicollinarity (meaning: high correlatons between the features, making them unsuitable for analysis)\n",
    "corr = prediktiv.drop('target', axis = 1)\n",
    "sns.heatmap(corr.corr(), center = 0);\n",
    "abs(corr.corr()) > 0.8\n",
    "\n",
    "#I want to keep the features with a correlation that is lower than 0.8\n",
    "#This means that I will keep the features that is 'false' in the table below\n",
    "#The diagonal 'True' is the correlation between each feature to themselves \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There was 4 features with high correlation among them and I will drop theese features\n",
    "prediktiv.drop(['feature01', 'feature02', 'feature05', 'feature07'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets to plot the features that are left to search for outliers\n",
    "\n",
    "fig, ax = plt.subplots(12, figsize = (15,6))\n",
    "ax[0].scatter(x = prediktiv['feature03'], y = prediktiv['target'])\n",
    "ax[1].scatter(x = prediktiv['feature04'], y = prediktiv['target'])\n",
    "ax[2].scatter(x = prediktiv['feature06'], y = prediktiv['target'])\n",
    "ax[3].scatter(x = prediktiv['feature08'], y = prediktiv['target'])\n",
    "ax[4].scatter(x = prediktiv['feature09'], y = prediktiv['target'])\n",
    "ax[5].scatter(x = prediktiv['feature10.1'], y = prediktiv['target'])\n",
    "ax[6].scatter(x = prediktiv['feature14'], y = prediktiv['target'])\n",
    "ax[7].scatter(x = prediktiv['feature17'], y = prediktiv['target'])\n",
    "ax[8].scatter(x = prediktiv['feature18'], y = prediktiv['target'])\n",
    "ax[9].scatter(x = prediktiv['feature19'], y = prediktiv['target'])\n",
    "ax[10].scatter(x = prediktiv['feature20'], y = prediktiv['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers (based on the scatterplots) that is bigger than 3 std. \n",
    "\n",
    "from scipy import stats\n",
    "prediktiv = prediktiv[(np.abs(stats.zscore(prediktiv.feature03)) < 3)]\n",
    "prediktiv = prediktiv[(np.abs(stats.zscore(prediktiv.feature04)) < 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will do the same for the target so the assumtions for regression-models is fulfilled\n",
    "#An assumption for regression-models is that the data should be normalised. \n",
    "#Here the target-feature is used with the log-function\n",
    "#This step might be skipped if the model is not a regression since the log 'pushes' the data to a normal-distribution, which sometimes is not necessary.\n",
    "prediktiv = prediktiv[(np.abs(stats.zscore(prediktiv.target)) < 3)]\n",
    "prediktiv.target = np.log(prediktiv.target)\n",
    "fig, axes = plt.subplots(2, figsize =(10, 6))\n",
    "sns.distplot(ax = axes[0], a =prediktiv.target);\n",
    "sns.boxplot(ax = axes[1], x = prediktiv.target);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is some outliers in target as well, I will get rid of that in the same way as for the other features\n",
    "prediktiv = prediktiv[(np.abs(stats.zscore(prediktiv.target)) < 3)]\n",
    "fig, axes = plt.subplots(2, figsize =(10, 6))\n",
    "sns.distplot(ax = axes[0], a =prediktiv.target);\n",
    "sns.boxplot(ax = axes[1], x = prediktiv.target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediktiv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is 26 features lef\n",
    "#The categorical variables are dummys\n",
    "#There is no NAN-values left in the dataset\n",
    "#There is no multicollinarity between the features\n",
    "\n",
    "#This fulfills the assumptions for my models:\n",
    "#There has to be homoskedasticity (homogenic variance within each feature)\n",
    "#There can not be any correaltion between the features (multicollinarity)\n",
    "#If the features has a normal distribution one can make assupmtions about the paramters and it will be easier to work with\n",
    "#The target y has to be in some way correlated with the feature x, which was displayed in the scatterplots.\n",
    "\n",
    "#I want to use the 10 best features in my models:\n",
    "top_features = prediktiv.corr().loc['target'].apply(np.abs).sort_values(ascending=False).index[1:11]\n",
    "top_features = list(top_features)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models using the ten best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prediktiv[top_features]\n",
    "y = prediktiv['target']\n",
    "robust_scaler = RobustScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 124)\n",
    "\n",
    "#This is a holdout cross validation which keeps 20% of the original data as \"testdata\", which is kept to evaluate the model.\n",
    "# The 80% that is not test-data is used for training the model to find the right parameter and prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a new dataframe 'df' for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = ['MSE', 'RMSE', 'MAE'], \n",
    "                       columns = ['NULL', 'MLR', 'KNN', 'LASSO'])\n",
    "\n",
    "\n",
    "#MSE is short for Mean squared Error\n",
    "#This is the squared average error between each observed value and it estimated value \n",
    "#MSE is the sum of al theese errors, which means that if the MSE is low: the model is good \n",
    "\n",
    "#RMSE is short for Root mean squared error\n",
    "#It is what it sounds like, the root of MSE.\n",
    "#RMSE has the same scale as the observation since it is not squared \n",
    "#As in MSE, it is the sum of errors and therefore a low value is good\n",
    "\n",
    "\n",
    "#MAE is short for Mean absolute error\n",
    "#This is the average error between the observed value and estimated value\n",
    "#MAE is not squared, and has the same scale as the observations \n",
    "#MAE < RMSE, since RMSE uses already squared values\n",
    "#RMSE is therefore desirable when there might be big errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the NULL-modell which predicts the average of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_null_modell = y_train.mean()\n",
    "df.loc['MSE', 'NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null_modell, y_train.size), y_true = y_train)\n",
    "df.loc['MSE', 'NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null_modell, y_test.size), y_true = y_test)\n",
    "\n",
    "df.loc['RMSE','NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null_modell, y_train.size), y_true=y_train, squared=False)\n",
    "df.loc['RMSE','NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null_modell, y_test.size), y_true=y_test, squared=False)\n",
    "\n",
    "df.loc['MAE','NULL'] = mean_absolute_error(y_pred=np.repeat(y_pred_null_modell, y_train.size), y_true=y_train)\n",
    "df.loc['MAE','NULL'] = mean_absolute_error(y_pred=np.repeat(y_pred_null_modell, y_test.size), y_true=y_test)\n",
    "print(y_pred_null_modell)\n",
    "\n",
    "\n",
    "#This is the average predicted value for y when y_train. One can compare this value with the test-model to see the actual average value in y_test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)\n",
    "\n",
    "df.loc['MSE', 'MLR'] = mean_squared_error(y_pred=lin_reg.predict(x_train), y_true=y_train)\n",
    "df.loc['MSE', 'MLR'] = mean_squared_error(y_pred=lin_reg.predict(x_test), y_true = y_test)\n",
    "\n",
    "df.loc['RMSE','MLR'] = mean_squared_error(y_pred=lin_reg.predict(x_train), y_true=y_train, squared = False)\n",
    "df.loc['RMSE','MLR'] = mean_squared_error(y_pred=lin_reg.predict(x_test), y_true=y_test, squared = False)\n",
    "\n",
    "df.loc['MAE','MLR'] = mean_absolute_error(y_pred=lin_reg.predict(x_train), y_true=y_train)\n",
    "df.loc['MAE','MLR'] = mean_absolute_error(y_pred=lin_reg.predict(x_test), y_true=y_test)\n",
    "\n",
    "\n",
    "#MLR are used to describe the relationships between features, dependent and independent\n",
    "#The MLR are used to predict the target-value for certain independent variables\n",
    "\n",
    "#https://www.scribbr.com/statistics/multiple-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the KNN (K-nearest neighbor)-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors = 10, weights = 'distance', metric = 'euclidean')\n",
    "knn_model.fit(x_train, y_train)\n",
    "\n",
    "df.loc['MSE','KNN'] = mean_squared_error(y_pred=knn_model.predict(x_train), y_true=y_train)\n",
    "df.loc['MSE','KNN'] = mean_squared_error(y_pred=knn_model.predict(x_test), y_true=y_test)\n",
    "\n",
    "df.loc['RMSE','KNN'] = mean_squared_error(y_pred=knn_model.predict(x_train), y_true=y_train, squared = False)\n",
    "df.loc['RMSE','KNN'] = mean_squared_error(y_pred=knn_model.predict(x_test), y_true=y_test, squared = False)\n",
    "\n",
    "df.loc['MAE','KNN'] = mean_absolute_error(y_pred=knn_model.predict(x_train), y_true=y_train)\n",
    "df.loc['MAE','KNN'] = mean_absolute_error(y_pred=knn_model.predict(x_test), y_true=y_test)\n",
    "\n",
    "#Knn is a cluster-method which uses the k nearest value \n",
    "#In a regression the model predicts the best output value from the nearest values in point\n",
    "#In this way one can make predictions from choosed distance, in this case the euclidean.\n",
    "#Connection the predictions creates a regression and the MSE etc. \n",
    "#A high k-value avoids overfitting but it can also lose some predictive power on the margins \n",
    "\n",
    "#https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, the Lasso-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_model = Lasso(alpha=0.05)\n",
    "lasso_model.fit(x_train, y_train)\n",
    "\n",
    "df.loc['MSE','LASSO'] = mean_squared_error(y_pred=lasso_model.predict(x_train), y_true=y_train)\n",
    "df.loc['MSE','LASSO'] = mean_squared_error(y_pred=lasso_model.predict(x_test), y_true=y_test)\n",
    "\n",
    "df.loc['RMSE','LASSO'] = mean_squared_error(y_pred=lasso_model.predict(x_train), y_true=y_train, squared= False)\n",
    "df.loc['RMSE','LASSO'] = mean_squared_error(y_pred=lasso_model.predict(x_test), y_true=y_test, squared = False)\n",
    "\n",
    "df.loc['MAE','LASSO'] = mean_absolute_error(y_pred=lasso_model.predict(x_train), y_true=y_train)\n",
    "df.loc['MAE','LASSO'] = mean_absolute_error(y_pred=lasso_model.predict(x_test), y_true=y_test)\n",
    "\n",
    "\n",
    "#Lasso-model are shrinking the values towards a central point.\n",
    "#The regression 'punishes' features with high correaltion and sets them to 0\n",
    "#Large error means that the model might eliminate some coefficients\n",
    "#Since the ten best features has been choosen for the model, lasso might not be needed\n",
    "\n",
    "print('Features choosen by Lasso:\\n')\n",
    "for i, var in enumerate(x.columns[lasso_model.coef_>0]):\n",
    "    print(\"{}.{}\".format(i+1, var))\n",
    "\n",
    "# https://www.statisticshowto.com/lasso-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "df.T.plot(kind='barh', ax=ax)\n",
    "ax.set_title('Error metrics for NULL, MLR, KNN and LASSO-models');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In each of theese models the outcome is based on finding the ordinary least squares.\n",
    "#This means finding the value between the observed and estimated value\n",
    "#This value is an error, as explained before, and this error must be minimized for the model to be considered as good.\n",
    "\n",
    "\n",
    "#For this data the lowest value was predicted in the multiple regression model, which tells us that this model has the best prediction for our target Y ('target'), based on selected features.\n",
    "#The LASSO- and KNN-model did also predict small value for MSE but failed at RMSE and MAE, which has significantly higher values\n",
    "#Lasso decided to keep only 5 of the features, which also must be included in the analysis since this is only half of the selected features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70ceab521e42877243433637ea59da6452e62acd4fc88782c3ef53963b2e77ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('environ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
